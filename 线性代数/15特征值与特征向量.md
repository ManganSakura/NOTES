## 1. 核心定义 (The Definition)

对于 $n \times n$ 的方阵 $A$，如果存在非零向量 $\mathbf{x}$ 和标量 $\lambda$，使得：

$$A \mathbf{x} = \lambda \mathbf{x}$$

- **$\mathbf{x}$** 称为 $A$ 的**特征向量 (Eigenvector)**。
    
- **$\lambda$** 称为 $A$ 的**特征值 (Eigenvalue)**。
    

### 几何意义

- 大部分向量 $\mathbf{y}$ 在乘以 $A$ 后，方向会改变（旋转）。
    
- **特征向量 $\mathbf{x}$** 在乘以 $A$ 后，**方向保持不变**（或者完全反向），只是长度变成了原来的 $\lambda$ 倍。
    
    - $\lambda > 1$：拉伸。
        
    - $0 < \lambda < 1$：压缩。
        
    - $\lambda < 0$：反向拉伸/压缩。
        

---

## 2. 求解步骤 (How to Calculate)

求解过程分为两步：先求 $\lambda$，再求 $\mathbf{x}$。

### 第一步：求特征值 $\lambda$

移项得到 $(A - \lambda I)\mathbf{x} = \mathbf{0}$。

因为我们要找非零解 $\mathbf{x}$，所以系数矩阵 $A - \lambda I$ 必须是奇异的 (Singular)。

$$\det(A - \lambda I) = 0$$

- 这叫**特征方程 (Characteristic Equation)**。
    
- 这是一个关于 $\lambda$ 的 $n$ 次多项式，会有 $n$ 个根（包含重根和复根）。
    

### 第二步：求特征向量 $\mathbf{x}$

对于每一个求出的 $\lambda_i$，代回方程：

$$(A - \lambda_i I) \mathbf{x} = \mathbf{0}$$

- 这本质上就是求矩阵 $A - \lambda_i I$ 的**零空间 (Null Space)**。
    
- 解出的基向量就是特征向量。
    

---

## 3. 两个“作弊”性质 (Trace and Determinant)

在算出 $\lambda$ 后，怎么知道自己算对没？看这两个性质，**必考！**

1. **迹 (Trace)**：特征值之和 = 对角线元素之和。
    
    $$\sum \lambda_i = \text{tr}(A) = a_{11} + a_{22} + \dots + a_{nn}$$
    
2. **行列式 (Determinant)**：特征值之积 = 行列式。
    
    $$\prod \lambda_i = \det(A)$$
    

> **技巧**：解出 $\lambda$ 后，立刻用 $2 \times 2$ 矩阵试一下这两个性质，如果不满足，肯定是算错了。

---

## 4. 特殊矩阵的特征值 (Special Matrices)

### A. 对角阵与三角阵

- **结论**：特征值就是**对角线上的元素**。
    
- 例子：$A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}$，$\lambda_1=3, \lambda_2=2$。
    

### B. 投影矩阵 (Projection Matrix, $P$)

- $P^2 = P$。
    
- **特征值**：只能是 **1** 或 **0**。
    
    - $\lambda=1$：对应于投影平面内的向量（投影后不变）。
        
    - $\lambda=0$：对应于垂直于平面的向量（投影后没了）。
        

### C. 奇异矩阵 (Singular Matrix)

- **结论**：至少有一个特征值为 **0**。
    
    - 因为 $\prod \lambda_i = \det(A) = 0$。
        

### D. 旋转矩阵 (Rotation Matrix, $Q$)

- 旋转 $90^\circ$：$Q = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$。
    
- **特征值**：$\lambda = i, -i$（复数）。
    
- **意义**：实空间中没有向量能“旋转 $90^\circ$ 后方向不变”。只有在复空间里才能找到解。
    

---

## 5. 对角化 (Diagonalization) —— 核心应用 $A = S \Lambda S^{-1}$

如果我们找到了 $n$ 个**线性无关**的特征向量，把它们作为列组成矩阵 $S$，把对应的特征值放在对角阵 $\Lambda$ 中。

$$A S = A \begin{bmatrix} \mathbf{x}_1 & \dots & \mathbf{x}_n \end{bmatrix} = \begin{bmatrix} \lambda_1 \mathbf{x}_1 & \dots & \lambda_n \mathbf{x}_n \end{bmatrix} = S \Lambda$$

右乘 $S^{-1}$ 得到：

$$A = S \Lambda S^{-1}$$

- **前提条件**：$A$ 必须有 **$n$ 个线性无关的特征向量**。
    
    - 如果 $n$ 个特征值**互不相同**，则一定可以对角化。
        
    - 如果有重根（如 $\lambda_1 = \lambda_2 = 3$），则**不一定**能对角化（取决于几何重数是否等于代数重数）。
        

### 为什么要对角化？—— 矩阵的幂

计算 $A^{100}$ 极其痛苦，但算 $\Lambda^{100}$ 极其简单。

$$A^k = (S \Lambda S^{-1})(S \Lambda S^{-1})\dots = S \Lambda^k S^{-1}$$

这是解决差分方程 $\mathbf{u}_{k+1} = A \mathbf{u}_k$ 的终极工具。

---

## 6. 国内考试常见陷阱

1. **$AB$ 和 $BA$ 的特征值**：
    
    - 它们拥有**相同**的特征值（除了 $0$ 可能不同）。
        
2. **$A+I$ 的特征值**：
    
    - 如果 $A$ 的特征值是 $\lambda$，那么 $A+kI$ 的特征值是 $\lambda + k$。特征向量**不变**。
        
3. **$A^2$ 的特征值**：
    
    - 是 $\lambda^2$。特征向量**不变**。
        

---

接下来的方向：

特征值的舞台搭好后，通常会有两个分支：

1. **应用篇**：微分方程 ($d\mathbf{u}/dt = A\mathbf{u}$)，这也是 $e^{At}$ 的来源。
    
2. **对称矩阵篇**：如果是对称矩阵 $A=A^T$，会有什么神奇的事情发生？（提示：特征值全是实数，特征向量正交）。
    

按照 Strang 的节奏，通常是先讲**微分方程**，再讲**对称矩阵**。您想按什么顺序？